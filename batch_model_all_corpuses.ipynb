{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188a9498",
   "metadata": {},
   "source": [
    "## PRELIMINARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aff328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 100)\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "pio.renderers.default='iframe'\n",
    "import hdbscan\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "os.environ[\"HUGGINGFACE_CO_TIMEOUT\"] = \"600\"  # Increase timeout duration to 600 seconds\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d719cfba",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b28142",
   "metadata": {},
   "source": [
    "### Define a function to clean the text in each csv file to ensure preprocessed text is properly cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to uniformly clean the text field of each csv file\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    # text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Remove inverted exclamation and question marks\n",
    "    text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
    "    \n",
    "    # Remove « and »\n",
    "    text = text.replace(\"«\", \"\").replace(\"»\", \"\")\n",
    "    \n",
    "    # Remove dashes\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    \n",
    "    # Remove quote marks\n",
    "    text = text.replace(\"'\", \"\").replace('\"', \"\")\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # Lower case\n",
    "    # text = text.lower()\n",
    "    \n",
    "    # Remove new lines and line breaks\n",
    "    # text = text.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350561a4",
   "metadata": {},
   "source": [
    "### Define a function to iterate through all the csv files in a directory and model them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a00cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_file(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    #print(df.columns)\n",
    "    #print(df.head(10))\n",
    "    \n",
    "\n",
    "   # Apply the cleaning function to the \"text\" column\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda text: clean_text(text))\n",
    "    print(df.columns)\n",
    "    print(df.head(10))\n",
    "\n",
    "#    # Drop the column named \"0\" if it exists\n",
    "#     if '0' in df.columns:\n",
    "#         df = df.drop(columns=['0'])\n",
    "        \n",
    "    # Check if the DataFrame has at least one row after cleaning\n",
    "    if len(df) > 0:\n",
    "        docs = df['text']\n",
    "\n",
    "        ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)\n",
    "        hdbscan_model = HDBSCAN(min_cluster_size=7, prediction_data=True)\n",
    "        umap_model = UMAP(n_neighbors=5, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "        sentence_model = SentenceTransformer(\"./dfm-sentence-encoder-medium-2\")\n",
    "\n",
    "        embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "        topic_model = BERTopic(\n",
    "            language='danish',\n",
    "            calculate_probabilities=True,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            embedding_model=sentence_model,\n",
    "            top_n_words=10,\n",
    "            min_topic_size=10,\n",
    "            ctfidf_model=ctfidf_model,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        topics, probs = topic_model.fit_transform(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f862a2d",
   "metadata": {},
   "source": [
    "The following lines can be uncommented or commented depending on what one wants to achieve--create non-stopword n-grams, apply an outlier reduction step using thresholded cTFIDF or probabilities, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # new_topics = topics\n",
    "        \n",
    "        # Use the \"c-TF-IDF\" strategy with a threshold\n",
    "        # new_topics = topic_model.reduce_outliers(docs, topics, strategy=\"c-tf-idf\", threshold=0.1)\n",
    "\n",
    "        # Reduce all outliers that are left with the \"probabilities\" strategy\n",
    "        # new_topics = topic_model.reduce_outliers(docs, new_topics, strategy=\"probabilities\", probabilities=probs)\n",
    "\n",
    "        # Apply count vectorizer to remove stopwords and create n-grams in the model\n",
    "        with open('./danish_stopwords.txt', 'r') as f:\n",
    "            danish_stop_words = f.read().splitlines()\n",
    "            vectorizer_model = CountVectorizer(stop_words=danish_stop_words, ngram_range=(1, 4), min_df=0.05)\n",
    "\n",
    "        # Update topics with the new vectorizer_model\n",
    "        topic_model.update_topics(docs, topics=topics, vectorizer_model=vectorizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aa2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "        # Get the filename without the extension\n",
    "        csv_filename = os.path.splitext(os.path.basename(csv_file_path))[0]\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        output_dir = './danish_regular_models/'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Print first ten topics\n",
    "        topic_model.get_topic_info()\n",
    "        \n",
    "        # Print model name and the number of topics\n",
    "        num_topics = len(topic_model.get_topic_freq())\n",
    "        \n",
    "        # Save the model and embeddings\n",
    "        print('The model has:', num_topics, \"topics.\")\n",
    "        print(f'Saving topic model and embeddings as: {csv_filename}_topic_model and {csv_filename}_embeddings.npy in {output_dir} using pickle')\n",
    "        embedding_model = \"./dfm-sentence-encoder-medium-2\"\n",
    "        # topic_model.save(f'{output_dir}/{csv_filename}_topic_model', serialization=\"safetensors\", save_ctfidf=True, save_embedding_model='./dfm-sentence-encoder-medium-2')\n",
    "        topic_model.save(f'{output_dir}/{csv_filename}_topic_model', serialization=\"pickle\")\n",
    "        np.save(f'{output_dir}{csv_filename}_embeddings.npy', embeddings)\n",
    "\n",
    "        # Print model name and the number of topics\n",
    "        num_topics = len(topic_model.get_topic_freq())\n",
    "        \n",
    "##        get the representative docs from the topic model and save as a dataframe--use for safetensors or pytorch\n",
    "#         rep_docs_df = topic_model.get_representative_docs()\n",
    "#         rep_docs_df = pd.DataFrame(rep_docs_df)\n",
    "#         rep_docs_df.to_csv(f'{output_dir}/{csv_filename}_repdocs.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c556b71",
   "metadata": {},
   "source": [
    "### Iterate through the files for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011193e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '[directory containing csv files]' with path to CSV files directory\n",
    "input_directory = './small'\n",
    "csv_files = [file for file in os.listdir(input_directory) if file.endswith('.csv')]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    csv_file_path = os.path.join(input_directory, csv_file)\n",
    "    print(f\"Now modeling: {csv_file}\")\n",
    "    process_csv_file(csv_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bertopic]",
   "language": "python",
   "name": "conda-env-bertopic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
