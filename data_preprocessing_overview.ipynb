{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151b594c",
   "metadata": {},
   "source": [
    "Sample code for preprocessing the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b1125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e81241a",
   "metadata": {},
   "source": [
    "Preliminary setup. Include nltk tokenizer, for other downstream tasks. BERTopic makes use of sckit.learn tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f29c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file_path):\n",
    "    df = pd.DataFrame(columns=['title', 'text'])\n",
    "    file_list = sorted(os.listdir(file_path))\n",
    "    for f in file_list:\n",
    "        dir = file_path + '/' + f\n",
    "        text = open(dir, 'r').read().strip()\n",
    "        df = df.append({'title':f[:-4], 'text':text}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b32f50",
   "metadata": {},
   "source": [
    "Simple chunking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5652f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking_text(chunk_size, text_list): \n",
    "    chunk_list = []\n",
    "    for text in text_list:\n",
    "        tokenized = word_tokenize(re.sub(r'[^\\w\\s]', '', text))\n",
    "        chunked = []\n",
    "        for i in range(0, len(tokenized), chunk_size):\n",
    "            chunked.append(' '.join(tokenized[i:i+chunk_size]))\n",
    "        chunk_list.append(chunked)\n",
    "    return chunk_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da030646",
   "metadata": {},
   "source": [
    "Load example dataset, here the tales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35227551",
   "metadata": {},
   "outputs": [],
   "source": [
    "tale_danish_df = load_text('fairytale_datasets/tales')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc0f6a",
   "metadata": {},
   "source": [
    "Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2e19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tale_danish_df['text'] = tale_danish_df['text'].replace(regex=r'\\s+', value=' ').replace(regex='[»«]', value='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2bd875",
   "metadata": {},
   "source": [
    "Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "394e317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tale_danish_df['text'] = chunking_text(50, tale_danish_df['text'])\n",
    "tale_danish_df = tale_danish_df.explode('text', ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd6d615",
   "metadata": {},
   "source": [
    "Save as csv file for future work. Do this for each subcorpus--travel writing, each of the four travel collections, and (as done here), the tales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a865034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tale_danish_df.to_csv(\"fairytale_datasets/tale_danish.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b1461",
   "metadata": {},
   "source": [
    "Create the various time-based bins\n",
    "\n",
    "- Group 1: 1835 - 1841 (after publication of Rambles in the Harz Mountains in 1831)\n",
    "- Group 2: 1842 - 1850 (after publication of A Poet's Bazaar in 1842)\n",
    "- Group 3: 1851 - 1862 (after publication of In Sweden in 1851)\n",
    "- Group 4: 1863 - 1873 (after publication of In Spain in 1863 and A Visit to Portugal in 1868)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c8ac36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_titles = tale_dates_dan['title'][tale_dates_dan['date'] <= 1841]\n",
    "group2_titles = tale_dates_dan['title'][(tale_dates_dan['date'] >= 1842) & (tale_dates_dan['date'] <= 1850)]\n",
    "group3_titles = tale_dates_dan['title'][(tale_dates_dan['date'] >= 1851) & (tale_dates_dan['date'] <= 1862)]\n",
    "group4_titles = tale_dates_dan['title'][tale_dates_dan['date'] >= 1863]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ec136f",
   "metadata": {},
   "source": [
    "Remember when creating the travel_tale corpus to manually delete the duplicate tales, e.g. Metalsvinet and En Rose fra Homers Grav."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "841834e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_skygge = travel_tale_danish_df[(travel_tale_danish_df['book'].isin(group1_titles)) | \n",
    "                                      (travel_tale_danish_df['book'] == 'Skygge')]\n",
    "group2_bazar = travel_tale_danish_df[(travel_tale_danish_df['book'].isin(group2_titles)) | \n",
    "                                     (travel_tale_danish_df['book'] == 'Bazar')]\n",
    "group3_sverrig = travel_tale_danish_df[(travel_tale_danish_df['book'].isin(group3_titles)) | \n",
    "                                       (travel_tale_danish_df['book'] == 'Sverrig')]\n",
    "group4_spanien = travel_tale_danish_df[(travel_tale_danish_df['book'].isin(group4_titles)) | \n",
    "                                      (travel_tale_danish_df['book'] == 'Spanien')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c50b19",
   "metadata": {},
   "source": [
    "Save the binned groups to csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea093d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_skygge.to_csv(\"both_datasets/group1_skygge.csv\")\n",
    "group2_bazar.to_csv(\"both_datasets/group2_bazar.csv\")\n",
    "group3_sverrig.to_csv(\"both_datasets/group3_sverrig.csv\")\n",
    "group4_spanien.to_csv(\"both_datasets/group4_spanien.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
